{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wNjDKdQy35h"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRm-USlsHgEV",
        "outputId": "ff8d9500-fa0a-4954-fd21-20a251d1ebf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-CycleGAN-and-pix2pix'...\n",
            "remote: Enumerating objects: 2513, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 2513 (delta 0), reused 5 (delta 0), pack-reused 2508\u001b[K\n",
            "Receiving objects: 100% (2513/2513), 8.20 MiB | 18.09 MiB/s, done.\n",
            "Resolving deltas: 100% (1573/1573), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HyperSimon/pytorch-CycleGAN-and-pix2pix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Pt3igws3eiVp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('pytorch-CycleGAN-and-pix2pix/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1EySlOXwwoa"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8daqlgVhw29P"
      },
      "source": [
        "# 通过挂载drive获取数据\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrdOettJxaCc",
        "outputId": "cc243203-dfbd-461c-d3f2-5c48ccd81a92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdUz4116xhpm"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# 此内容为代码格式\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC2DEP4M0OsS"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# 打开文件\n",
        "with open('../drive/MyDrive/checkpoints/chain-saw-man/loss_log.txt', 'r') as f:\n",
        "    # 将文件内容读取到列表中\n",
        "    lines = f.readlines()\n",
        "\n",
        "# 获取最后一行并去除空白字符\n",
        "last_line = lines[-1].strip()\n",
        "\n",
        "# 使用正则表达式提取 epoch 数据\n",
        "epoch = re.findall(r\"epoch:\\s*(\\d+)\", last_line)[0]\n",
        "\n",
        "print(epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFw1kDQBx3LN"
      },
      "source": [
        "# Training\n",
        "\n",
        "-   `python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA`\n",
        "\n",
        "Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. Add `--direction BtoA` if you want to train a model to transfrom from class B to A."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sp7TCT2x9dB",
        "outputId": "539a5aaf-5f78-4a03-bc68-55a6412e5cbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- Options ---------------\n",
            "               batch_size: 1                             \n",
            "                    beta1: 0.5                           \n",
            "          checkpoints_dir: ../drive/MyDrive/checkpoints  \t[default: ./checkpoints]\n",
            "           continue_train: True                          \t[default: False]\n",
            "                crop_size: 256                           \n",
            "                 dataroot: ../drive/MyDrive/chain-saw-man\t[default: None]\n",
            "             dataset_mode: aligned                       \n",
            "                direction: AtoB                          \n",
            "              display_env: main                          \n",
            "             display_freq: 400                           \n",
            "               display_id: -1                            \t[default: 1]\n",
            "            display_ncols: 4                             \n",
            "             display_port: 8097                          \n",
            "           display_server: http://localhost              \n",
            "          display_winsize: 256                           \n",
            "                    epoch: latest                        \n",
            "              epoch_count: 216                           \t[default: 1]\n",
            "                 gan_mode: vanilla                       \n",
            "                  gpu_ids: 0                             \n",
            "                init_gain: 0.02                          \n",
            "                init_type: normal                        \n",
            "                 input_nc: 3                             \n",
            "                  isTrain: True                          \t[default: None]\n",
            "                lambda_L1: 100.0                         \n",
            "                load_iter: 0                             \t[default: 0]\n",
            "                load_size: 286                           \n",
            "                       lr: 0.0002                        \n",
            "           lr_decay_iters: 50                            \n",
            "                lr_policy: linear                        \n",
            "         max_dataset_size: inf                           \n",
            "                    model: pix2pix                       \t[default: cycle_gan]\n",
            "                 n_epochs: 20000                         \t[default: 100]\n",
            "           n_epochs_decay: 100                           \n",
            "               n_layers_D: 3                             \n",
            "                     name: chain-saw-man                 \t[default: experiment_name]\n",
            "                      ndf: 64                            \n",
            "                     netD: basic                         \n",
            "                     netG: unet_256                      \n",
            "                      ngf: 64                            \n",
            "               no_dropout: False                         \n",
            "                  no_flip: False                         \n",
            "                  no_html: False                         \n",
            "                     norm: batch                         \n",
            "              num_threads: 4                             \n",
            "                output_nc: 3                             \n",
            "                    phase: train                         \n",
            "                pool_size: 0                             \n",
            "               preprocess: resize_and_crop               \n",
            "               print_freq: 100                           \n",
            "             save_by_iter: False                         \n",
            "          save_epoch_freq: 5                             \n",
            "         save_latest_freq: 5000                          \n",
            "           serial_batches: False                         \n",
            "                   suffix:                               \n",
            "         update_html_freq: 1000                          \n",
            "                use_wandb: False                         \n",
            "                  verbose: False                         \n",
            "       wandb_project_name: CycleGAN-and-pix2pix          \n",
            "----------------- End -------------------\n",
            "dataset [AlignedDataset] was created\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "The number of training images = 161\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "model [Pix2PixModel] was created\n",
            "loading the model from ../drive/MyDrive/checkpoints/chain-saw-man/latest_net_G.pth\n",
            "loading the model from ../drive/MyDrive/checkpoints/chain-saw-man/latest_net_D.pth\n",
            "---------- Networks initialized -------------\n",
            "[Network G] Total number of parameters : 54.414 M\n",
            "[Network D] Total number of parameters : 2.769 M\n",
            "-----------------------------------------------\n",
            "create web directory ../drive/MyDrive/checkpoints/chain-saw-man/web...\n",
            "/usr/local/lib/python3.9/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 216, iters: 100, time: 0.134, data: 2.290) G_GAN: 3.440 G_L1: 11.012 D_real: 0.162 D_fake: 0.591 \n",
            "End of epoch 216 / 20100 \t Time Taken: 76 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 217, iters: 39, time: 0.063, data: 0.820) G_GAN: 2.307 G_L1: 14.379 D_real: 0.253 D_fake: 0.719 \n",
            "(epoch: 217, iters: 139, time: 0.097, data: 0.001) G_GAN: 2.388 G_L1: 12.016 D_real: 0.146 D_fake: 0.487 \n",
            "End of epoch 217 / 20100 \t Time Taken: 56 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 218, iters: 78, time: 3.487, data: 0.001) G_GAN: 3.146 G_L1: 15.380 D_real: 0.110 D_fake: 0.504 \n",
            "End of epoch 218 / 20100 \t Time Taken: 58 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 219, iters: 17, time: 0.068, data: 0.011) G_GAN: 2.593 G_L1: 12.846 D_real: 0.158 D_fake: 0.985 \n",
            "(epoch: 219, iters: 117, time: 0.071, data: 0.001) G_GAN: 1.134 G_L1: 8.987 D_real: 0.666 D_fake: 0.315 \n",
            "End of epoch 219 / 20100 \t Time Taken: 58 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 220, iters: 56, time: 0.089, data: 0.001) G_GAN: 1.010 G_L1: 9.946 D_real: 0.701 D_fake: 0.346 \n",
            "(epoch: 220, iters: 156, time: 2.263, data: 0.393) G_GAN: 1.464 G_L1: 11.367 D_real: 0.440 D_fake: 0.911 \n",
            "saving the model at the end of epoch 220, iters 805\n",
            "End of epoch 220 / 20100 \t Time Taken: 60 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 221, iters: 95, time: 0.088, data: 0.007) G_GAN: 1.278 G_L1: 11.970 D_real: 0.286 D_fake: 0.269 \n",
            "End of epoch 221 / 20100 \t Time Taken: 54 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 222, iters: 34, time: 0.083, data: 0.006) G_GAN: 1.800 G_L1: 15.572 D_real: 0.237 D_fake: 0.249 \n",
            "(epoch: 222, iters: 134, time: 0.093, data: 0.001) G_GAN: 1.362 G_L1: 13.342 D_real: 0.784 D_fake: 0.080 \n",
            "End of epoch 222 / 20100 \t Time Taken: 55 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 223, iters: 73, time: 1.601, data: 0.001) G_GAN: 1.546 G_L1: 13.921 D_real: 0.399 D_fake: 0.292 \n",
            "End of epoch 223 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 224, iters: 12, time: 0.088, data: 0.004) G_GAN: 0.978 G_L1: 13.729 D_real: 0.720 D_fake: 0.358 \n",
            "(epoch: 224, iters: 112, time: 0.087, data: 0.792) G_GAN: 1.587 G_L1: 8.882 D_real: 0.593 D_fake: 0.498 \n",
            "End of epoch 224 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 225, iters: 51, time: 0.114, data: 0.931) G_GAN: 2.916 G_L1: 14.162 D_real: 0.034 D_fake: 0.292 \n",
            "(epoch: 225, iters: 151, time: 1.807, data: 0.042) G_GAN: 3.667 G_L1: 20.728 D_real: 0.022 D_fake: 0.079 \n",
            "saving the model at the end of epoch 225, iters 1610\n",
            "End of epoch 225 / 20100 \t Time Taken: 58 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 226, iters: 90, time: 0.068, data: 0.001) G_GAN: 2.056 G_L1: 17.090 D_real: 0.088 D_fake: 0.989 \n",
            "End of epoch 226 / 20100 \t Time Taken: 55 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 227, iters: 29, time: 0.067, data: 0.001) G_GAN: 1.620 G_L1: 12.285 D_real: 0.543 D_fake: 0.149 \n",
            "(epoch: 227, iters: 129, time: 0.099, data: 0.001) G_GAN: 2.148 G_L1: 14.542 D_real: 0.067 D_fake: 1.871 \n",
            "End of epoch 227 / 20100 \t Time Taken: 54 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 228, iters: 68, time: 2.513, data: 0.007) G_GAN: 1.738 G_L1: 10.524 D_real: 0.409 D_fake: 0.826 \n",
            "End of epoch 228 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 229, iters: 7, time: 0.119, data: 0.004) G_GAN: 1.700 G_L1: 11.094 D_real: 0.147 D_fake: 0.604 \n",
            "(epoch: 229, iters: 107, time: 0.165, data: 0.001) G_GAN: 2.445 G_L1: 17.251 D_real: 0.188 D_fake: 0.190 \n",
            "End of epoch 229 / 20100 \t Time Taken: 55 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 230, iters: 46, time: 0.101, data: 0.001) G_GAN: 2.248 G_L1: 15.293 D_real: 0.102 D_fake: 0.466 \n",
            "(epoch: 230, iters: 146, time: 1.296, data: 0.001) G_GAN: 1.872 G_L1: 12.924 D_real: 0.463 D_fake: 0.595 \n",
            "saving the model at the end of epoch 230, iters 2415\n",
            "End of epoch 230 / 20100 \t Time Taken: 55 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 231, iters: 85, time: 0.068, data: 0.001) G_GAN: 1.739 G_L1: 22.573 D_real: 0.009 D_fake: 1.862 \n",
            "End of epoch 231 / 20100 \t Time Taken: 56 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 232, iters: 24, time: 0.094, data: 0.001) G_GAN: 3.392 G_L1: 10.804 D_real: 0.038 D_fake: 1.087 \n",
            "(epoch: 232, iters: 124, time: 0.077, data: 0.694) G_GAN: 1.885 G_L1: 12.514 D_real: 0.258 D_fake: 0.359 \n",
            "End of epoch 232 / 20100 \t Time Taken: 52 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 233, iters: 63, time: 2.981, data: 0.650) G_GAN: 1.299 G_L1: 11.500 D_real: 0.249 D_fake: 0.980 \n",
            "End of epoch 233 / 20100 \t Time Taken: 54 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 234, iters: 2, time: 0.141, data: 0.001) G_GAN: 0.740 G_L1: 11.453 D_real: 1.056 D_fake: 0.130 \n",
            "(epoch: 234, iters: 102, time: 0.109, data: 0.001) G_GAN: 2.410 G_L1: 19.564 D_real: 0.000 D_fake: 1.359 \n",
            "End of epoch 234 / 20100 \t Time Taken: 54 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 235, iters: 41, time: 0.071, data: 0.002) G_GAN: 1.709 G_L1: 10.935 D_real: 0.471 D_fake: 0.283 \n",
            "(epoch: 235, iters: 141, time: 1.712, data: 0.119) G_GAN: 2.877 G_L1: 16.360 D_real: 0.028 D_fake: 0.321 \n",
            "saving the model at the end of epoch 235, iters 3220\n",
            "End of epoch 235 / 20100 \t Time Taken: 55 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 236, iters: 80, time: 0.069, data: 0.006) G_GAN: 1.508 G_L1: 10.776 D_real: 0.411 D_fake: 0.770 \n",
            "End of epoch 236 / 20100 \t Time Taken: 54 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 237, iters: 19, time: 0.127, data: 0.189) G_GAN: 1.008 G_L1: 13.229 D_real: 0.788 D_fake: 0.153 \n",
            "(epoch: 237, iters: 119, time: 0.115, data: 0.001) G_GAN: 1.802 G_L1: 13.969 D_real: 0.344 D_fake: 0.556 \n",
            "End of epoch 237 / 20100 \t Time Taken: 52 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 238, iters: 58, time: 1.298, data: 0.002) G_GAN: 2.161 G_L1: 23.272 D_real: 0.016 D_fake: 0.243 \n",
            "(epoch: 238, iters: 158, time: 0.085, data: 0.001) G_GAN: 1.205 G_L1: 13.290 D_real: 3.377 D_fake: 0.011 \n",
            "End of epoch 238 / 20100 \t Time Taken: 55 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 239, iters: 97, time: 0.077, data: 0.597) G_GAN: 2.897 G_L1: 12.925 D_real: 0.890 D_fake: 0.034 \n",
            "End of epoch 239 / 20100 \t Time Taken: 52 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 240, iters: 36, time: 0.102, data: 0.932) G_GAN: 1.707 G_L1: 10.320 D_real: 0.446 D_fake: 0.859 \n",
            "(epoch: 240, iters: 136, time: 2.341, data: 0.279) G_GAN: 2.024 G_L1: 11.603 D_real: 0.372 D_fake: 0.816 \n",
            "saving the model at the end of epoch 240, iters 4025\n",
            "End of epoch 240 / 20100 \t Time Taken: 57 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 241, iters: 75, time: 0.113, data: 0.007) G_GAN: 1.393 G_L1: 12.500 D_real: 0.361 D_fake: 0.303 \n",
            "End of epoch 241 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 242, iters: 14, time: 0.089, data: 0.001) G_GAN: 1.084 G_L1: 12.034 D_real: 1.337 D_fake: 0.146 \n",
            "(epoch: 242, iters: 114, time: 0.088, data: 0.001) G_GAN: 2.448 G_L1: 12.586 D_real: 0.119 D_fake: 0.998 \n",
            "End of epoch 242 / 20100 \t Time Taken: 57 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 243, iters: 53, time: 1.075, data: 0.001) G_GAN: 2.117 G_L1: 15.268 D_real: 0.281 D_fake: 0.181 \n",
            "(epoch: 243, iters: 153, time: 0.088, data: 0.001) G_GAN: 1.268 G_L1: 13.660 D_real: 0.426 D_fake: 0.373 \n",
            "End of epoch 243 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 244, iters: 92, time: 0.097, data: 0.920) G_GAN: 0.741 G_L1: 9.761 D_real: 0.943 D_fake: 0.227 \n",
            "End of epoch 244 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 245, iters: 31, time: 0.096, data: 0.546) G_GAN: 1.921 G_L1: 13.227 D_real: 0.358 D_fake: 0.256 \n",
            "(epoch: 245, iters: 131, time: 1.576, data: 0.001) G_GAN: 1.263 G_L1: 11.175 D_real: 0.552 D_fake: 0.311 \n",
            "saving the model at the end of epoch 245, iters 4830\n",
            "End of epoch 245 / 20100 \t Time Taken: 56 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 246, iters: 70, time: 0.100, data: 0.008) G_GAN: 2.275 G_L1: 12.251 D_real: 0.075 D_fake: 1.098 \n",
            "End of epoch 246 / 20100 \t Time Taken: 54 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 247, iters: 9, time: 0.064, data: 0.001) G_GAN: 1.521 G_L1: 12.932 D_real: 0.270 D_fake: 0.447 \n",
            "saving the latest model (epoch 247, total_iters 5000)\n",
            "(epoch: 247, iters: 109, time: 0.065, data: 0.003) G_GAN: 0.739 G_L1: 15.393 D_real: 0.553 D_fake: 0.098 \n",
            "End of epoch 247 / 20100 \t Time Taken: 55 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 248, iters: 48, time: 1.937, data: 0.068) G_GAN: 0.543 G_L1: 13.549 D_real: 1.385 D_fake: 0.216 \n",
            "(epoch: 248, iters: 148, time: 0.068, data: 0.006) G_GAN: 1.064 G_L1: 13.156 D_real: 1.025 D_fake: 0.398 \n",
            "End of epoch 248 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 249, iters: 87, time: 0.099, data: 0.566) G_GAN: 0.836 G_L1: 9.510 D_real: 2.146 D_fake: 0.053 \n",
            "End of epoch 249 / 20100 \t Time Taken: 54 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 250, iters: 26, time: 0.082, data: 0.000) G_GAN: 1.656 G_L1: 9.366 D_real: 0.857 D_fake: 0.153 \n",
            "(epoch: 250, iters: 126, time: 2.742, data: 0.100) G_GAN: 1.412 G_L1: 13.619 D_real: 0.366 D_fake: 0.348 \n",
            "saving the model at the end of epoch 250, iters 5635\n",
            "End of epoch 250 / 20100 \t Time Taken: 55 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 251, iters: 65, time: 0.080, data: 0.001) G_GAN: 0.528 G_L1: 13.722 D_real: 0.789 D_fake: 0.149 \n",
            "End of epoch 251 / 20100 \t Time Taken: 56 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 252, iters: 4, time: 0.099, data: 0.463) G_GAN: 1.632 G_L1: 13.545 D_real: 0.526 D_fake: 0.178 \n",
            "(epoch: 252, iters: 104, time: 0.115, data: 0.756) G_GAN: 0.498 G_L1: 12.013 D_real: 1.388 D_fake: 0.237 \n",
            "End of epoch 252 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 253, iters: 43, time: 1.723, data: 0.759) G_GAN: 1.544 G_L1: 12.268 D_real: 0.607 D_fake: 0.142 \n",
            "(epoch: 253, iters: 143, time: 0.094, data: 0.001) G_GAN: 1.829 G_L1: 12.606 D_real: 0.393 D_fake: 0.403 \n",
            "End of epoch 253 / 20100 \t Time Taken: 55 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 254, iters: 82, time: 0.141, data: 0.001) G_GAN: 0.928 G_L1: 11.370 D_real: 0.723 D_fake: 0.335 \n",
            "End of epoch 254 / 20100 \t Time Taken: 52 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 255, iters: 21, time: 0.107, data: 0.242) G_GAN: 1.719 G_L1: 13.180 D_real: 0.587 D_fake: 0.147 \n",
            "(epoch: 255, iters: 121, time: 1.080, data: 0.001) G_GAN: 1.875 G_L1: 11.547 D_real: 0.116 D_fake: 0.333 \n",
            "saving the model at the end of epoch 255, iters 6440\n",
            "End of epoch 255 / 20100 \t Time Taken: 56 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 256, iters: 60, time: 0.101, data: 0.013) G_GAN: 0.555 G_L1: 9.311 D_real: 0.840 D_fake: 0.273 \n",
            "(epoch: 256, iters: 160, time: 0.091, data: 0.930) G_GAN: 2.141 G_L1: 14.960 D_real: 0.168 D_fake: 0.519 \n",
            "End of epoch 256 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 257, iters: 99, time: 0.094, data: 0.196) G_GAN: 1.456 G_L1: 10.565 D_real: 1.093 D_fake: 0.114 \n",
            "End of epoch 257 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 258, iters: 38, time: 2.755, data: 0.001) G_GAN: 1.005 G_L1: 8.939 D_real: 0.698 D_fake: 0.242 \n",
            "(epoch: 258, iters: 138, time: 0.081, data: 0.009) G_GAN: 1.269 G_L1: 12.921 D_real: 0.705 D_fake: 0.361 \n",
            "End of epoch 258 / 20100 \t Time Taken: 54 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 259, iters: 77, time: 0.097, data: 0.276) G_GAN: 1.548 G_L1: 10.359 D_real: 0.577 D_fake: 0.594 \n",
            "End of epoch 259 / 20100 \t Time Taken: 52 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 260, iters: 16, time: 0.084, data: 0.187) G_GAN: 1.527 G_L1: 12.906 D_real: 0.821 D_fake: 0.098 \n",
            "(epoch: 260, iters: 116, time: 1.568, data: 0.375) G_GAN: 2.223 G_L1: 17.003 D_real: 0.035 D_fake: 0.837 \n",
            "saving the model at the end of epoch 260, iters 7245\n",
            "End of epoch 260 / 20100 \t Time Taken: 56 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 261, iters: 55, time: 0.090, data: 0.001) G_GAN: 1.869 G_L1: 14.257 D_real: 0.063 D_fake: 0.563 \n",
            "(epoch: 261, iters: 155, time: 0.105, data: 0.001) G_GAN: 1.369 G_L1: 9.904 D_real: 0.877 D_fake: 0.115 \n",
            "End of epoch 261 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 262, iters: 94, time: 0.097, data: 0.000) G_GAN: 2.940 G_L1: 15.230 D_real: 0.097 D_fake: 0.739 \n",
            "End of epoch 262 / 20100 \t Time Taken: 56 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 263, iters: 33, time: 2.260, data: 0.003) G_GAN: 3.472 G_L1: 14.773 D_real: 0.025 D_fake: 1.729 \n",
            "(epoch: 263, iters: 133, time: 0.099, data: 0.015) G_GAN: 1.690 G_L1: 10.000 D_real: 0.350 D_fake: 0.567 \n",
            "End of epoch 263 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 264, iters: 72, time: 0.108, data: 0.862) G_GAN: 1.141 G_L1: 11.763 D_real: 0.671 D_fake: 0.359 \n",
            "End of epoch 264 / 20100 \t Time Taken: 55 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 265, iters: 11, time: 0.090, data: 0.710) G_GAN: 1.329 G_L1: 11.746 D_real: 0.713 D_fake: 0.249 \n",
            "(epoch: 265, iters: 111, time: 1.638, data: 0.001) G_GAN: 0.996 G_L1: 9.991 D_real: 1.186 D_fake: 0.449 \n",
            "saving the model at the end of epoch 265, iters 8050\n",
            "End of epoch 265 / 20100 \t Time Taken: 55 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 266, iters: 50, time: 0.130, data: 0.001) G_GAN: 2.715 G_L1: 12.270 D_real: 0.235 D_fake: 1.004 \n",
            "(epoch: 266, iters: 150, time: 0.076, data: 0.001) G_GAN: 1.876 G_L1: 11.148 D_real: 0.226 D_fake: 0.762 \n",
            "End of epoch 266 / 20100 \t Time Taken: 56 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 267, iters: 89, time: 0.078, data: 0.001) G_GAN: 1.119 G_L1: 10.033 D_real: 0.561 D_fake: 0.289 \n",
            "End of epoch 267 / 20100 \t Time Taken: 52 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 268, iters: 28, time: 2.396, data: 0.143) G_GAN: 1.117 G_L1: 11.655 D_real: 1.192 D_fake: 0.073 \n",
            "(epoch: 268, iters: 128, time: 0.099, data: 0.002) G_GAN: 0.884 G_L1: 10.534 D_real: 0.961 D_fake: 0.229 \n",
            "End of epoch 268 / 20100 \t Time Taken: 55 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 269, iters: 67, time: 0.107, data: 0.001) G_GAN: 1.957 G_L1: 9.902 D_real: 0.210 D_fake: 0.825 \n",
            "End of epoch 269 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 270, iters: 6, time: 0.062, data: 0.001) G_GAN: 1.955 G_L1: 11.458 D_real: 0.165 D_fake: 0.528 \n",
            "(epoch: 270, iters: 106, time: 2.335, data: 0.000) G_GAN: 0.976 G_L1: 10.668 D_real: 0.623 D_fake: 0.351 \n",
            "saving the model at the end of epoch 270, iters 8855\n",
            "End of epoch 270 / 20100 \t Time Taken: 57 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 271, iters: 45, time: 0.064, data: 0.001) G_GAN: 1.246 G_L1: 10.129 D_real: 0.437 D_fake: 0.347 \n",
            "(epoch: 271, iters: 145, time: 0.054, data: 0.024) G_GAN: 1.787 G_L1: 11.705 D_real: 0.267 D_fake: 0.419 \n",
            "End of epoch 271 / 20100 \t Time Taken: 54 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 272, iters: 84, time: 0.093, data: 0.641) G_GAN: 1.199 G_L1: 11.604 D_real: 0.755 D_fake: 0.226 \n",
            "End of epoch 272 / 20100 \t Time Taken: 54 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 273, iters: 23, time: 2.064, data: 0.841) G_GAN: 1.685 G_L1: 12.132 D_real: 0.219 D_fake: 0.177 \n",
            "(epoch: 273, iters: 123, time: 0.070, data: 0.003) G_GAN: 1.460 G_L1: 10.044 D_real: 0.531 D_fake: 0.317 \n",
            "End of epoch 273 / 20100 \t Time Taken: 54 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 274, iters: 62, time: 0.107, data: 0.001) G_GAN: 1.071 G_L1: 9.728 D_real: 0.854 D_fake: 0.182 \n",
            "End of epoch 274 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 275, iters: 1, time: 0.140, data: 0.445) G_GAN: 1.145 G_L1: 10.638 D_real: 0.557 D_fake: 0.538 \n",
            "(epoch: 275, iters: 101, time: 2.680, data: 0.149) G_GAN: 2.632 G_L1: 15.728 D_real: 0.330 D_fake: 0.129 \n",
            "saving the model at the end of epoch 275, iters 9660\n",
            "End of epoch 275 / 20100 \t Time Taken: 57 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 276, iters: 40, time: 0.098, data: 0.004) G_GAN: 1.066 G_L1: 10.641 D_real: 2.241 D_fake: 0.078 \n",
            "(epoch: 276, iters: 140, time: 0.098, data: 0.001) G_GAN: 1.684 G_L1: 11.909 D_real: 0.202 D_fake: 0.504 \n",
            "End of epoch 276 / 20100 \t Time Taken: 55 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 277, iters: 79, time: 0.071, data: 0.001) G_GAN: 1.690 G_L1: 9.581 D_real: 0.226 D_fake: 0.422 \n",
            "End of epoch 277 / 20100 \t Time Taken: 54 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 278, iters: 18, time: 2.672, data: 0.458) G_GAN: 1.233 G_L1: 10.394 D_real: 0.643 D_fake: 0.354 \n",
            "saving the latest model (epoch 278, total_iters 10000)\n",
            "(epoch: 278, iters: 118, time: 0.116, data: 0.001) G_GAN: 1.143 G_L1: 12.001 D_real: 1.448 D_fake: 0.044 \n",
            "End of epoch 278 / 20100 \t Time Taken: 55 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 279, iters: 57, time: 0.062, data: 1.503) G_GAN: 1.891 G_L1: 14.377 D_real: 0.418 D_fake: 0.094 \n",
            "(epoch: 279, iters: 157, time: 0.099, data: 0.194) G_GAN: 1.661 G_L1: 11.667 D_real: 0.363 D_fake: 0.257 \n",
            "End of epoch 279 / 20100 \t Time Taken: 56 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 280, iters: 96, time: 4.656, data: 0.706) G_GAN: 1.733 G_L1: 9.901 D_real: 0.360 D_fake: 0.422 \n",
            "saving the model at the end of epoch 280, iters 10465\n",
            "End of epoch 280 / 20100 \t Time Taken: 64 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 281, iters: 35, time: 0.088, data: 0.010) G_GAN: 1.551 G_L1: 14.303 D_real: 0.384 D_fake: 0.389 \n",
            "(epoch: 281, iters: 135, time: 0.074, data: 0.002) G_GAN: 1.845 G_L1: 16.177 D_real: 0.148 D_fake: 0.368 \n",
            "End of epoch 281 / 20100 \t Time Taken: 59 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 282, iters: 74, time: 0.063, data: 0.001) G_GAN: 0.768 G_L1: 9.835 D_real: 1.005 D_fake: 0.289 \n",
            "End of epoch 282 / 20100 \t Time Taken: 54 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 283, iters: 13, time: 2.219, data: 0.418) G_GAN: 1.568 G_L1: 15.192 D_real: 0.555 D_fake: 0.191 \n",
            "(epoch: 283, iters: 113, time: 0.099, data: 0.009) G_GAN: 3.731 G_L1: 9.108 D_real: 0.064 D_fake: 1.509 \n",
            "End of epoch 283 / 20100 \t Time Taken: 59 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 284, iters: 52, time: 0.100, data: 0.344) G_GAN: 1.816 G_L1: 10.523 D_real: 0.278 D_fake: 0.455 \n",
            "(epoch: 284, iters: 152, time: 0.068, data: 0.935) G_GAN: 2.179 G_L1: 23.059 D_real: 0.198 D_fake: 0.162 \n",
            "End of epoch 284 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 285, iters: 91, time: 2.111, data: 0.664) G_GAN: 2.085 G_L1: 10.083 D_real: 0.283 D_fake: 0.651 \n",
            "saving the model at the end of epoch 285, iters 11270\n",
            "End of epoch 285 / 20100 \t Time Taken: 59 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 286, iters: 30, time: 0.102, data: 0.001) G_GAN: 1.335 G_L1: 11.127 D_real: 0.844 D_fake: 0.389 \n",
            "(epoch: 286, iters: 130, time: 0.095, data: 0.001) G_GAN: 2.267 G_L1: 10.179 D_real: 0.064 D_fake: 0.979 \n",
            "End of epoch 286 / 20100 \t Time Taken: 55 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 287, iters: 69, time: 0.073, data: 0.001) G_GAN: 2.243 G_L1: 15.074 D_real: 0.306 D_fake: 0.581 \n",
            "End of epoch 287 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 288, iters: 8, time: 2.471, data: 0.002) G_GAN: 1.474 G_L1: 10.146 D_real: 0.280 D_fake: 0.372 \n",
            "(epoch: 288, iters: 108, time: 0.066, data: 0.003) G_GAN: 1.687 G_L1: 10.749 D_real: 0.294 D_fake: 0.800 \n",
            "End of epoch 288 / 20100 \t Time Taken: 52 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 289, iters: 47, time: 0.136, data: 1.274) G_GAN: 3.327 G_L1: 11.902 D_real: 0.135 D_fake: 0.866 \n",
            "(epoch: 289, iters: 147, time: 0.096, data: 0.000) G_GAN: 1.683 G_L1: 15.194 D_real: 0.133 D_fake: 0.542 \n",
            "End of epoch 289 / 20100 \t Time Taken: 52 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 290, iters: 86, time: 1.720, data: 0.747) G_GAN: 1.931 G_L1: 13.196 D_real: 0.201 D_fake: 0.440 \n",
            "saving the model at the end of epoch 290, iters 12075\n",
            "End of epoch 290 / 20100 \t Time Taken: 55 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 291, iters: 25, time: 0.067, data: 0.001) G_GAN: 0.930 G_L1: 12.737 D_real: 0.571 D_fake: 0.183 \n",
            "(epoch: 291, iters: 125, time: 0.073, data: 0.804) G_GAN: 1.579 G_L1: 11.075 D_real: 0.695 D_fake: 0.119 \n",
            "End of epoch 291 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 292, iters: 64, time: 0.072, data: 1.050) G_GAN: 0.801 G_L1: 11.022 D_real: 1.099 D_fake: 0.134 \n",
            "End of epoch 292 / 20100 \t Time Taken: 54 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 293, iters: 3, time: 2.242, data: 0.001) G_GAN: 0.612 G_L1: 10.738 D_real: 0.967 D_fake: 0.318 \n",
            "(epoch: 293, iters: 103, time: 0.110, data: 0.001) G_GAN: 2.647 G_L1: 13.721 D_real: 0.098 D_fake: 0.738 \n",
            "End of epoch 293 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 294, iters: 42, time: 0.080, data: 0.091) G_GAN: 3.916 G_L1: 15.953 D_real: 0.073 D_fake: 1.652 \n",
            "(epoch: 294, iters: 142, time: 0.095, data: 0.001) G_GAN: 1.633 G_L1: 9.970 D_real: 1.159 D_fake: 0.062 \n",
            "End of epoch 294 / 20100 \t Time Taken: 54 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 295, iters: 81, time: 1.230, data: 0.001) G_GAN: 1.321 G_L1: 13.395 D_real: 0.299 D_fake: 0.161 \n",
            "saving the model at the end of epoch 295, iters 12880\n",
            "End of epoch 295 / 20100 \t Time Taken: 56 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 296, iters: 20, time: 0.106, data: 0.009) G_GAN: 1.875 G_L1: 9.545 D_real: 0.289 D_fake: 0.431 \n",
            "(epoch: 296, iters: 120, time: 0.108, data: 0.865) G_GAN: 1.283 G_L1: 11.761 D_real: 0.858 D_fake: 0.362 \n",
            "End of epoch 296 / 20100 \t Time Taken: 53 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n"
          ]
        }
      ],
      "source": [
        "!python train.py --dataroot ../drive/MyDrive/chain-saw-man \\\n",
        " --name chain-saw-man \\\n",
        " --model pix2pix \\\n",
        " --display_id -1 \\\n",
        " --checkpoints_dir ../drive/MyDrive/checkpoints \\\n",
        " --continue_train \\\n",
        " --n_epochs 20000 \\\n",
        " --epoch_count 216"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UkcaFZiyASl"
      },
      "source": [
        "# Testing\n",
        "\n",
        "-   `python test.py --dataroot ./datasets/facades --direction BtoA --model pix2pix --name facades_pix2pix`\n",
        "\n",
        "Change the `--dataroot`, `--name`, and `--direction` to be consistent with your trained model's configuration and how you want to transform images.\n",
        "\n",
        "> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\n",
        "> Note that we specified --direction BtoA as Facades dataset's A to B direction is photos to labels.\n",
        "\n",
        "> If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use --model test option. See ./scripts/test_single.sh for how to apply a model to Facade label maps (stored in the directory facades/testB).\n",
        "\n",
        "> See a list of currently available models at ./scripts/download_pix2pix_model.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mey7o6j-0368"
      },
      "outputs": [],
      "source": [
        "!ls checkpoints/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCsKkEq0yGh0"
      },
      "outputs": [],
      "source": [
        "!python test.py --dataroot ./datasets/facades --direction BtoA --model pix2pix --name facades_label2photo_pretrained --use_wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzSKIPUByfiN"
      },
      "source": [
        "# Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Mgg8raPyizq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_fake_B.png')\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G3oVH9DyqLQ"
      },
      "outputs": [],
      "source": [
        "img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_real_A.png')\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErK5OC1j1LH4"
      },
      "outputs": [],
      "source": [
        "img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_real_B.png')\n",
        "plt.imshow(img)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "pix2pix",
      "provenance": []
    },
    "environment": {
      "name": "tf2-gpu.2-3.m74",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
